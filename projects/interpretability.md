---
layout: notes
title: Interpretability
---
Modern machine learning models, particulary deep neural networks, are difficult to understand and explain. This has consequences when these networks are used to make critical decisions. Thus it is important to develop methods that can interpret these highly complex models.